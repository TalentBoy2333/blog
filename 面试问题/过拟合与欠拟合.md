我的理解: 其实，问到"样本数据过少怎么办", 其实也可以用过拟合这一套来回答. </br>
总之, 最重要的, `回答之前先分析问题(what, why, how)`: </br>
## 什么过拟合/欠拟合
在我们机器学习和深度学习的训练过程中, 经常会出现过拟合和欠拟合的现象. 训练一开始, 模型通常会欠拟合, 所以会对模型进行优化, 然而等到训练到一定程度的时候, 就需要解决过拟合的问题了. </br>
如何判断过拟合呢? 我们在训练过程中会定义训练误差, 验证集误差, 测试集误差(泛化误差). 训练误差总是减少的, 而泛化误差一开始会减少, 但到一定程序后不减反而增加, 这时候便出现了过拟合的现象. </br>
个人理解: 在深度学习中会出现过拟合的根本原因是所谓模型的`VC维`太高, 而数据量过小, 这种不匹配导致了拟合能力强大的神经网络将不需要学习的偏差算在了需要学习的内容里面, 导致了过拟合. 说白了就是参数太多, 数据太少, 那么解决过拟合的核心思路就是`减少参数`或者`增加数据量`. </br>

## 产生过拟合的原因
* 训练集和测试集特征分布不一致
* 数据噪声太大
* 数据量太小
* 特征量太多
* 模型太过复杂

在回答时可以分类别来说，这样可以显得比较有条理: </br>
从`数据分布`角度分析, 可能是因为`训练集和测试集特征分布不一致`或者是`数据噪声太大`, 导致模型没有完美地拟合训练集与测试集的数据分布. </br>
从`VC维`的角度分析，可能是因为`数据量太小`, `特征量太多`或者`模型太过复杂`导致的模型拟合能力过强, 过度学习了训练集的数据分布, 而不符合测试集. </br>

## 解决过拟合的方法
#### 方法1 - early stopping
对模型进行训练的过程即是对模型的参数进行学习更新的过程, 这个参数学习的过程往往会用到一些迭代方法, 如梯度下降(Gradient descent)学习算法. Early stopping便是一种迭代次数截断的方法来防止过拟合的方法, 即在模型对训练数据集迭代收敛之前停止迭代来防止过拟合. </br>

Early stopping方法的具体做法是, 在每一个Epoch结束时(一个Epoch集为对所有的训练数据的一轮遍历)计算validation data的accuracy,当accuracy不再提高时, 就停止训练. 这种做法很符合直观感受, 因为accurary都不再提高了, 在继续训练也是无益的, 只会提高训练的时间. 那么该做法的一个重点便是怎样才认为validation accurary不再提高了呢? 并不是说validation accuracy一降下来便认为不再提高了, 因为可能经过这个Epoch后, accuracy降低了, 但是随后的Epoch又让accuracy又上去了, 所以不能根据一两次的连续降低就判断不再提高. 一般的做法是, 在训练的过程中, 记录到目前为止最好的validation accuracy, 当连续10次Epoch(或者更多次)没达到最佳accuracy时, 则可以认为accuracy不再提高了. 此时便可以停止迭代了(Early Stopping). </br>

#### 方法2 - Data augmentation
以图片数据集举例, 能够做各种变换, 如:
* 将原始图片旋转一个小角度 
* 加入随机噪声
* 一些有弹性的畸变(elastic distortions), 论文"Best practices for convolutional neural networks applied to visual document analysis"对MNIST做了各种变种扩增. 
* 截取(crop)原始图片的一部分, 比方DeepID中, 从一副人脸图中, 截取出了100个小patch作为训练数据, 极大地添加了数据集. 

#### 方法3 - Loss函数加入正则项
正则化(Regularization)包含L1, L2(L2 regularization也叫权重衰减, weight decay). 
L1会趋向于产生少量的特征, 而其他的特征都是0, 而L2会选择更多的特征, 这些特征都会接近于0. L1正则在特征选择时候非常有用, 而L2正则就只是一种规则化而已. 
如前面所说, 解决过拟合的核心思路在于`减少参数`或者`增加数据量`, 而在Loss函数中加入正则项使得神经网络的参数大部分趋向于0, 换句话说正则化抑制神经网络的部分参数的激励, 某种意义上等价于`减少了神经网络的参数`. </br>

#### 方法4 - Dropout
同样的, Dropout本质上就是`减少神经网络的参数`. </br>

#### 方法5 - Batch Normalization
BN每次的mini-batch的数据都不一样, 但是每次的mini-batch的数据都会对moving mean和moving variance产生作用, 可以认为是引入了噪声, 这就可以认为是进行了`data augmentation`, 而`data augmentation`被认为是防止过拟合的一种方法. 因此, 可以认为用BN可以防止过拟合. </br>

#### 方法6 - 简化模型
简化神经网络模型, 或者用传统的机器学习模型(决策树, 朴素贝叶斯, SVM), 实质上也是在`减少神经网络的参数`. </br>

#### 方法7 - 交叉验证
这个需要特别说明, 某些算法除了需要学习的参数外, 还有一些需要人为设置的超参数(例如KNN和KMeans的`k`值，支持向量机的`C`值), 如果超参数设置的不好也会导致模型过拟合. 而交叉验证的作用，就是让我们进行多次的模型训练比较. 例如, 我们期望从`[ 1, 10, 100, 1000 ]`这几个值中选择一个最优值, 作为支持向量机的`C`值. 那么我们就得分别用这4个值进行10折交叉验证, 把每一次交叉验证结果的均值进行比较, 从而选择出一个最优值. 最优值选出来了, 那么模型的超参数确定了, 接下来, 利用全量的数据(不要划分训练集测试集, 而是用全部的数据)进行模型训练, 训练出来的模型才是我们的最终模型. </br>

## 总结
在回答的时候, 除了要先分析问题(what, why)之外, 在回答怎么解决(how)时, 还需要分类去说, 这样显得条理清晰, 比如, 从`数据`的角度, 我们提出`数据扩增`的方法. 从`优化过程`角度, 我们提出`early stopping`的方法. 从`模型算法`的角度, 我们提出`简化模型`, `Batch Normalization`, `Dropout`, `在Loss函数中加入正则项`等方法. 这样分别介绍感觉效果会好很多. </br>

## 参考文献
[1]https://www.jianshu.com/p/f8b86af75020
[2]https://blog.csdn.net/u010899985/article/details/79471909
[3]https://blog.csdn.net/u012019029/article/details/80023898
[4]https://www.cnblogs.com/solong1989/p/9415606.html
[5]https://www.zhihu.com/question/275788133/answer/384198714