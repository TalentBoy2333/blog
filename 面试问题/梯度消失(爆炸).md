首先, 分析为什么会发生梯度消失/爆炸: </br>
因为, 在进行`BP算法`的时候会反复计算: </br>
* 激活函数的偏导数
* `wx + b`的偏导数 

对于深层的神经网络, 越浅层的网络参数在进行迭代更新时, 需要计算的偏导数联乘就越多. </br>
若每一个梯度都很小, 联乘后就会得到一个极度小的数, 乘以学习率后进行参数更新的结果是几乎不更新, 这就是梯度消失. 同样的, 若每个梯度都很大, 则会得到一个极度大的数, 参数更新时会加上一个十分巨大的数, 失去了优化的意义, 这就是梯度爆炸. </br>
解决思路: </br>
* 使激活函数的偏导数不会消失(激活函数的梯度一般都在可控范围内, 不会爆炸)
* 使`wx + b`的偏导数不会爆炸/消失 

解决方法: </br>
#### 方案1-预训练加微调
此方法来自Hinton在2006年发表的一篇论文，Hinton为了解决梯度的问题，提出采取无监督逐层训练方法，其基本思想是每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，此过程就是逐层“预训练”（pre-training）；在预训练完成后，再对整个网络进行“微调”（fine-tunning）。Hinton在训练深度信念网络（Deep Belief Networks中，使用了这个方法，在各层预训练完成后，再利用BP算法对整个网络进行训练。此思想相当于是先寻找局部最优，然后整合起来寻找全局最优，此方法有一定的好处，但是目前应用的不是很多了。</br>

#### 方案2-梯度剪切、正则
梯度剪切这个方案主要是针对梯度爆炸提出的, 其思想是设置一个梯度剪切阈值, 然后更新梯度的时候, 如果梯度超过这个阈值, 那么就将其强制限制在这个范围之内. 这可以防止梯度爆炸. </br>
另外一种解决梯度爆炸的手段是采用权重正则化（weithts regularization）比较常见的是l1正则，和l2正则. </br>

#### 方案3-relu、leakrelu、elu等激活函数
`relu`函数的导数在正数部分是恒等于1的, 因此在深层网络中使用`relu`激活函数就不会导致梯度消失和爆炸的问题. </br>
* 解决了梯度消失/爆炸的问题
* 计算方便, 计算速度快 
* 加速了网络的训练 
</br>

#### 方案4-batchnorm
激活函数部分: </br>
batchnorm通过对每一层的输出做scale和shift的方法, 通过一定的规范化手段, 把每层神经网络任意神经元这个输入值的分布强行拉回到接近均值为0方差为1的标准正太分布, 即严重偏离的分布强制拉回比较标准的分布, 这样使得激活输入值落在非线性函数对输入比较敏感的区域.</br>
`wx + b`偏导数部分: </br>
batchnorm使得`BN(Wu)`与`BN(aWu)`对`u`的偏导数一致, 即神经网络权重变大/变小也不会导致梯度计算过大/过小. </br>

#### 方案5-残差结构
Resnet中的shortcut结构可以无损地传播梯度. </br>

#### 方案6-LSTM
LSTM全称是长短期记忆网络(long-short term memory networks), 是不那么容易发生梯度消失的, 主要原因在于LSTM内部复杂的"门"(gates). </br>

## 参考文献
[1]https://zhuanlan.zhihu.com/p/33006526 </br>
[2]S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of The 32nd International Conference on Ma- chine Learning, pages 448–456, 2015. </br>